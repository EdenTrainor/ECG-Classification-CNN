#!/usr/bin/env python3.5

#-------------- MODULE IMPORTS -----------------

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import tensorflow as tf 
import scipy as sp
import numpy as np
import time, os
import csv

tf.logging.set_verbosity(tf.logging.INFO)

def cnn_model_fn(features, labels, mode, params):

#----------- MODEL ARCHITECTURE -----------------
	
	
	input_layer = tf.reshape(features["x"], [-1, 1000, 1], name = "input_layer") #needs reshaping with new data
	
	#Convolutional Layer # 1	
	conv1 = tf.layers.conv1d(										
		inputs=input_layer,
		filters=80,						
		kernel_size=params["kernel_size"],			#Kernal size too small for data?
		padding="same",
		activation=tf.nn.relu,	#Relu allows for complete sparcity
		trainable = False,  	#Will intialise this with sparse encoded image maps!
		name = "conv1")   
			 
	#Average activations of convolutional layer 1
	with tf.variable_scope('Activations1'):
		average_density_1 = tf.reduce_mean(tf.reduce_sum(tf.cast((conv1 > 0), tf.float32), axis=[1]), name = "average_density_1")
		tf.summary.scalar('AvergageDensity1', average_density_1)
	
	#Pooling Layers #1
	pool1 = tf.layers.max_pooling1d(
		inputs=conv1,					 
		pool_size=2,					
		strides=2,
		name = "pool1")
	
	#Convolutional Layer # 2
	conv2 = tf.layers.conv1d(
		inputs=pool1,		
		filters=160,
		kernel_size=params["kernel_size"],
		padding="same",
		activation=tf.nn.leaky_relu,
		name = "conv2")
	
	#Log the average activations of the second layer
	with tf.variable_scope('Activations1'):
		average_density_2 = tf.reduce_mean(tf.reduce_sum(tf.cast((conv2 > 0), tf.float32), axis=[1]), name = "average_density_2")
		tf.summary.scalar('AvergageDensity2', average_density_2)
	
	#Pooling layer # 2

	pool2 = tf.layers.max_pooling1d(
		inputs=conv2,
		pool_size=2,
		strides=2,
		name = "pool2")
	
	#Dense Layer
	

	pool2_flat = tf.reshape(pool2, [-1, 160 * int(pool2.shape[1])])
	
	dense = tf.layers.dense(
		inputs=pool2_flat,
		units=128,
		activation=tf.nn.relu,
		name = "dense") #RELU NOT LEAKY!!!


		
	dropout = tf.layers.dropout(
		inputs=dense,
		rate=0.4,
		training = mode == tf.estimator.ModeKeys.TRAIN)
	
	
	#Logits layer
	
	logits = tf.layers.dense(
		inputs=dropout,
		units=2,
		name = "logits")
	
#--------------- MODEL OUTPUT STRUCTURES ----------------

	predictions = {

		#Generate Predictions (for PREDICT and EVAL mode)
		
		"classes": tf.argmax(input=logits, axis=1),
		
		#Add 'softmax_tensor' to the  graph. It is used for the
		#PREDICT by the 'logging_hook'
		
		"probabilities": tf.nn.softmax(logits, name="softmax_tensor")
	}
	

	with tf.variable_scope('Accuracy'):
		labelsOH = tf.one_hot(labels, 2)
		correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(logits), 1), tf.argmax(labelsOH, 1))
		accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
		
		#log the accuracy
		tf.summary.scalar('training_accuracy', accuracy)

	
	if mode == tf.estimator.ModeKeys.PREDICT:
		return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)
	
	#Calculate Loss
	
	with tf.variable_scope('Loss_Layer'):	
		loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
	
	#Create a logging hook for training metrics

	train_logging_hook = tf.train.SummarySaverHook(
		save_steps = 50,
		output_dir = params["dir"],
		summary_op = tf.summary.merge_all())

	
	#Load up the image maps for conv1 from a checkpoint of the sparse encoder
	tf.train.init_from_checkpoint(params["checkpoint"],
								{'conv1/kernel':'conv1/kernel', #This overrides default initialization ops of specified variables.
								'conv1/bias':'conv1/bias'})
	
	#Configure the training Op (for TRAIN mode)
	if mode == tf.estimator.ModeKeys.TRAIN:
		
		optimizer = tf.train.AdamOptimizer(learning_rate=1e-3) #CHANGED FOR SPARSITY MODEL
		train_op = optimizer.minimize(
			loss=loss, 
			global_step=tf.train.get_global_step())
		return tf.estimator.EstimatorSpec(
			mode=mode, 
			loss=loss, 
			train_op=train_op, 
			training_hooks = [train_logging_hook]) #ADDED LOGGING HOOK
	
	# Add evaluation metric (for EVAL mode), These give final performance metrics.
	
	eval_metric_ops = {
		"final_accuracy": tf.metrics.accuracy(
			labels=labels, predictions=predictions["classes"]), #Calculates how often the predictions matches the labels 
		
		"roc_auc_score": tf.metrics.auc( 
			labels = labels, predictions = predictions["classes"]), #Computes the approximate AUC via a Riemann sum
		
		"sensativity": tf.metrics.true_positives(
			labels = labels, predictions = predictions["classes"]), #Sum the weights of true-positives
		
		"false-positive (1 - specificity)": tf.metrics.false_positives(
			labels = labels, predictions = predictions["classes"]), #Sum the weights of false-positives
		
		"precision": tf.metrics.precision(
			labels = labels, predictions = predictions["classes"]) #Computes the precision of the predictions with respect to the labels.
	}
	

	return tf.estimator.EstimatorSpec(
		mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)

def main(unused_argv):

#------------------ DATA IMPORT --------------------	

	#File Directory
	dir = os.path.dirname(os.path.realpath(__file__))

	#File Name
	filename = str(os.path.basename(os.path.realpath(__file__))).split('.')[0]

	#Load ecgData:
	ecgData = np.load("./data/ecgData_std.npy")#ecgData
	##unseenData = np.load("./data/unseenData_std.npy")#unseenData

	#Load Class data
	classData = np.load("./data/classData_std.npy")
	##unseenClass = np.load("./data/unseenClass_std.npy")
	
	#load training data and eval data
	eval_data = ecgData[:round(ecgData.shape[0]*0.2)].astype(np.float32)
	train_data = ecgData[round(ecgData.shape[0]*0.2):].astype(np.float32)
	eval_labels = classData[:round(ecgData.shape[0]*0.2)].astype(np.int32)
	train_labels = classData[round(ecgData.shape[0]*0.2):].astype(np.int32)

#--------- NEURAL NETWORK OPERATIONS -------------------

	ksize = []
	sparsity = []
	accuracies = []
	losses = []
	auc = []
	sensativity = []
	specificity = []
	precision = []
	times = []
	
	
	for ker in [5, 15, 25, 35]:
		for sc in [0, 1e-5, 5e-5, 1e-4, 5e-4]: 
			
			model_dir = dir + '/Results/' + filename + '/ker-' + str(ker) + '/sc-' + str(sc) + '/'
			
			checkpoint = dir + '/Results/sparse_encoder_single_layer/ker-' + str(ker) + '/sc-' + str(sc) + '/'
			
			#Start performance counter
			start = time.perf_counter()

			#Make the results directory

			if not os.path.exists(model_dir):
				os.makedirs(model_dir)
		
			#Hyperparameters to pass to the

			model_params = {
							"sparcity_constraint": sc,
							"dir": model_dir,
							"checkpoint": checkpoint,
							"kernel_size": ker
							}

			#create the estimator
		
			ecg_classifier = tf.estimator.Estimator(
			model_fn=cnn_model_fn, model_dir= model_dir, params = model_params)

			#Set up logging for predictions
			tensors_to_log = {
				"probabilities": "softmax_tensor",
				}
		
			logging_hook = tf.train.LoggingTensorHook(
				tensors=tensors_to_log, every_n_iter=50)
		
			#Train the Model
			train_input_fn = tf.estimator.inputs.numpy_input_fn(
				x={"x": train_data},
				y=train_labels,
				batch_size=100,
				num_epochs=None,
				shuffle=True)
			
			ecg_classifier.train(
				input_fn=train_input_fn,
				steps=20000,
				hooks=[logging_hook])
		
			#Evaluate the model and print results
			eval_input_fn = tf.estimator.inputs.numpy_input_fn(
				x={"x": eval_data},
				y= eval_labels,
				num_epochs=1,
				shuffle=False)
		

			eval_results = ecg_classifier.evaluate(input_fn=eval_input_fn)
	
			print(eval_results)

#------------- RESULTS OUTPUTTING ----------------
			
			#Finish performance counter
			finish = time.perf_counter()
			train_time = finish - start
			print("Total traintime: %.3f seconds" % train_time)
		
			#with open(save_dir + 'CNN_results_summary.txt', 'a+') as f:
			#	f.write('sc-'+ str(sc) + ': %s\r\n' % (str(eval_results)))

			ksize.append(ker)
			sparsity.append(sc)
			accuracies.append(eval_results["final_accuracy"])
			losses.append(eval_results["loss"])
			times.append(train_time)
			precision.append(eval_results["precision"])
			auc.append(eval_results["roc_auc_score"])
			sensativity.append(eval_results["sensativity"])
			specificity.append(eval_results["false-positive (1 - specificity)"])

			print("Tensorboard logdir at: " + model_dir)
	
	#Write results to a csv file with a header in a particular order
	
	y = np.zeros([20,5])

	for i in  np.arange(20):
		y[i, 0] = ksize[i]
		y[i, 1] = sparsity[i]
		y[i, 2] = accuracies[i]
		y[i, 3] = losses[i]
		y[i, 4] = auc[i]
		y[i, 5] = sensativity[i]
		y[i, 6] = specificity[i]
		y[i, 7] = precision[i]
		y[i, 8] = times[i]

	with open(dir + '/Results/' + filename + '/results_summary-ECG-1D.csv', 'a+') as csvfile:
		w = csv.writer(csvfile)
		top_row = np.asarray(['Kernal Size', 'Sparcity Constraint', 'Accuracy', 'Loss', 'Roc Auc Score', 'Sensativity', '1 - Specificity', 'Precision' 'Training Time'])
		w.writerow(top_row)

	for j in range(20):
		with open(dir + '/Results/' + filename + '/results_summary-ECG-1D.csv', 'a+') as csvfile:
			results_writer = csv.writer(csvfile)
			results_writer.writerow(y[int(j),])

	print('Results:')

	with open(dir + '/Results/' + filename + '/results_summary-ECG-1D.csv', 'r') as csvfile:
		results_reader = csv.reader(csvfile)
		for row in results_reader:
			print(row)
			print()


if __name__ == "__main__":
	tf.app.run()
