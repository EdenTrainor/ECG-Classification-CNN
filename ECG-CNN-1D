#!/usr/bin/env python3.5

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import tensorflow as tf
import tflearn 
import scipy as sp
import numpy as np
import time, os

tf.logging.set_verbosity(tf.logging.INFO)

def cnn_model_fn(features, labels, mode, params):
	#Model function for CNN.
	
	
	input_layer = tf.reshape(features["x"], [-1, 1000, 1], name = "input_layer") #needs reshaping with new data
	
	#Convolutional Layer # 1
	
	conv1 = tf.layers.conv1d(										
		inputs=input_layer,
		filters=32,						
		kernel_size=5,					
		padding="same",
		activation=tf.nn.leaky_relu,
		training = False,  
		name = "conv1")   
			 
		#Average activations of convolutional layer 1
	
	average_density_1 = tf.reduce_mean(tf.reduce_sum(tf.cast((conv1 > 0), tf.float32), axis=[1]), name = "average_density_1")
	tf.summary.scalar('AvergageDensity1', average_density_1)
	
	#Pooling Layers #1
	pool1 = tf.layers.max_pooling1d(
		inputs=conv1,					 
		pool_size=2,					
		strides=2,
		name = "pool1")
	
	#Convolutional Layer # 2
	conv2 = tf.layers.conv1d(
		inputs=pool1,					
		filters=64,
		kernel_size=5,
		padding="same",
		activation=tf.nn.leaky_relu,
		name = "conv2")

	average_density_2 = tf.reduce_mean(tf.reduce_sum(tf.cast((conv2 > 0), tf.float32), axis=[1]), name = "average_density_2")
	tf.summary.scalar('AvergageDensity2', average_density_2)
	
	#Pooling layer # 2

	pool2 = tf.layers.max_pooling1d(
		inputs=conv2,
		pool_size=2,
		strides=2,
		name = "pool2")
	
	#Dense Layer
	

	pool2_flat = tf.reshape(pool2, [-1, 64 * int(pool2.shape[1])]) #needs reshaping with new data
	
	dense = tf.layers.dense(
		inputs=pool2_flat,
		units=128,
		activation=tf.nn.relu,
		name = "dense") 


		
	dropout = tf.layers.dropout(
		inputs=dense,
		rate=0.4,
		training = mode == tf.estimator.ModeKeys.TRAIN)
	
	
	#Logits layer
	
	logits = tf.layers.dense(
		inputs=dropout,
		units=2,
		name = "logits")
	
	
	predictions = {
		#Generate Predictions (for PREDICT and EVAL mode)
		
		"classes": tf.argmax(input=logits, axis=1),
		
		#Add 'softmax_tensor' to the  graph. It is used for the
		#PREDICT by the 'logging_hook'
		"probabilities": tf.nn.softmax(logits, name="softmax_tensor")
	}
	

	with tf.variable_scope('Accuracy'):
		labelsOH = tf.one_hot(labels, 2)
		correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(logits), 1), tf.argmax(labelsOH, 1))
		accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
		
		#log the accuracy
		tf.summary.scalar('Accuracy', accuracy)

	#Create a logging hook for training metrics


	if mode == tf.estimator.ModeKeys.PREDICT:
		return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)
	
	#Calculate Loss
	#onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32),depth=2)
	#with tf.name_scope("loss_tensor"): #ADDED NAME
	
	with tf.variable_scope('Loss_Layer'):	
		loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits, name = "loss")
	
	#Create a logging hook for training metrics

	train_logging_hook = tf.train.SummarySaverHook(
		save_steps = 50,
		output_dir = params["dir"],
		summary_op = tf.summary.merge_all())

	#Configure the training Op (for TRAIN mode)
	if mode == tf.estimator.ModeKeys.TRAIN:
		
		optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)
		train_op = optimizer.minimize(
			loss=loss, 
			global_step=tf.train.get_global_step())
		return tf.estimator.EstimatorSpec(
			mode=mode, 
			loss=loss, 
			train_op=train_op, 
			training_hooks = [train_logging_hook]) 
	
	# Add evaluation metric (for EVAL mode)
	
	eval_metric_ops = {
		"accuracy": tf.metrics.accuracy(
			labels=labels, predictions=predictions["classes"]), #Calculates how often the predictions matches the labels 
		
		"roc_auc_score": tf.metrics.auc( 
			labels = labels, predictions = predictions["classes"]), #Computes the approximate AUC via a Riemann sum
		
		"sensetivity": tf.metrics.true_positives(
			labels = labels, predictions = predictions["classes"]), #Sum the weights of true-positives
		
		"false-positive (1 - specificity)": tf.metrics.false_positives(
			labels = labels, predictions = predictions["classes"]), #Sum the weights of false-positives
		
		"precision": tf.metrics.precision(
			labels = labels, predictions = predictions["classes"]) #Computes the precision of the predictions with respect to the labels.
	}
	

	return tf.estimator.EstimatorSpec(
		mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)

def main(unused_argv):
	
	#File Directory
	dir = os.path.dirname(os.path.realpath(__file__))

	#File Name
	filename = str(os.path.basename(os.path.realpath(__file__)))

	#Load ecgData:
	ecgData = np.load("./data/ecgData_std.npy")#ecgData

	#Load Class data
	classData = np.load("./data/classData_std.npy")
	
	#load training data and eval data
	eval_data = ecgData[:round(ecgData.shape[0]*0.2)].astype(np.float32)
	train_data = ecgData[round(ecgData.shape[0]*0.2):].astype(np.float32)
	eval_labels = classData[:round(ecgData.shape[0]*0.2)].astype(np.int32)
	train_labels = classData[round(ecgData.shape[0]*0.2):].astype(np.int32)

	
	model_dir = dir +'/' + filename[:len(filename)-3] + '/Results/1D/' + str(int(time.time())) 
		
	#Make the results directory

	if not os.path.exists(model_dir):
		os.makedirs(model_dir)
		
	#Hyperparameters to pass to the

	model_params = {#"sparcity_constraint": sc,
					#"dir": model_dir
					}

	#create the estimator
	
	#Turn this on when optimizing for GPU 
	"""
	config = tf.ConfigProto()
	config.gpu_options.allow_growth = True #Allows the NN to grow the amount of GPU memory it uses
	config.gpu_options.per_process_gpu_memory_fraction = 0.6 #Sets a max amount of GPU memory usage 
															#as some is used for GUI etc  
	ecg_classifier =  tf.estimator.Estimator(config=tf.contrib.learn.RunConfig(session_config=config),
	model_fn=cnn_model_fn, model_dir="/tmp/ecg_convnet_model")
	"""
	#Use this when optmizing for CPU
		
	ecg_classifier = tf.estimator.Estimator(
	model_fn=cnn_model_fn, model_dir= model_dir, params = model_params)

	#Set up logging for predictions
	tensors_to_log = {
		"probabilities": "softmax_tensor",
		}
	
	logging_hook = tf.train.LoggingTensorHook(
		tensors=tensors_to_log, every_n_iter=50)
	
	#Train the Model
	train_input_fn = tf.estimator.inputs.numpy_input_fn(
		x={"x": train_data},
		y=train_labels,
		batch_size=100,
		num_epochs=None,
		shuffle=True)
		
	ecg_classifier.train(
		input_fn=train_input_fn,
		steps=20000,
		hooks=[logging_hook])
		
		#Evaluate the model and print results
	eval_input_fn = tf.estimator.inputs.numpy_input_fn(
		x={"x": eval_data},
		y= eval_labels,
		num_epochs=1,
		shuffle=False)
		

	eval_results = ecg_classifier.evaluate(input_fn=eval_input_fn)
	
	print(eval_results)
	print()
	print("Tensorboard logdir at: " + model_dir)



if __name__ == "__main__":
	tf.app.run()
